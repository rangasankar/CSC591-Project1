{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996646498695336\n",
      "0.00033523767075636815\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# File: activation.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "#\"\"\"The sigmoid function.\"\"\"\n",
    "    sigm = 1. / (1. + np.exp(-z))\n",
    "    return sigm\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "#    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    sigm = 1. / (1. + np.exp(-z))\n",
    "    sigm_prime = sigm * (1. - sigm)\n",
    "    return (sigm_prime)\n",
    "\n",
    "\n",
    "c = sigmoid(8)\n",
    "print(c)\n",
    "d = sigmoid_prime(8)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x, y, biases, weightsT, cost, num_layers):\n",
    "    \"\"\" function of backpropagation\n",
    "        Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient of all biases and weights.\n",
    "        Args:\n",
    "            x, y: input image x and label y\n",
    "            biases, weights (list): list of biases and transposed weights of entire network\n",
    "            cost (CrossEntropyCost): object of cost computation\n",
    "            num_layers (int): number of layers of the network\n",
    "        Returns:\n",
    "            (nabla_b, nabla_wT): tuple containing the gradient for all the biases\n",
    "                and weightsT. nabla_b and nabla_wT should be the same shape as \n",
    "                input biases and weightsT\n",
    "    \"\"\"\n",
    "    # initial zero list for store gradient of biases and weights\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_wT = [np.zeros(wT.shape) for wT in weightsT]\n",
    "\n",
    "    ### Implement here\n",
    "    # feedforward\n",
    "    # Here you need to store all the activations of all the units\n",
    "    # by feedforward pass\n",
    "    ###\n",
    "    \n",
    "#https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py\n",
    "    activation = x\n",
    "    activations = [x] \n",
    "        # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "\n",
    "    # compute the gradient of error respect to output\n",
    "    # activations[-1] is the list of activations of the output layer\n",
    "    delta = (cost).df_wrt_a(activations[-1], y)\n",
    "    \n",
    "    ### Implement here\n",
    "    # backward pass\n",
    "    # Here you need to implement the backward pass to compute the\n",
    "    # gradient for each weight and bias\n",
    "    ###\n",
    "    \n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "    for l in xrange(2, self.num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    return (nabla_b, nabla_w)\n",
    "#    return (nabla_b, nabla_wT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
